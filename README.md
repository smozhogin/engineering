# Конвейер Apache Airflow в задаче машинного обучения

## Архитектура проекта

### Расположение проекта в среде Linux Ubuntu 24.04

    app/ - директория для хранения разрабатываемых приложений в ОС
    ├── project/ - директория для хранения проектов
    │   └── medic_pipeline/ - корневая директория проекта medic_pipeline
    │       ├── config.ini - файл конфигурации с токенами, исключен из git
    │       ├── dags/ - директория для хранения DAG-файлов
    │       │   └── pipeline_dag.py
    │       ├── etl/ - директория для хранения ETL-файлов
    │       │   ├── data_loader.py - шаг 1 конвейера: загрузка данных
    │       │   ├── data_preprocessor.py - шаг 2 конвейера: предобработка данных
    │       │   ├── model_evaluator.py - - шаг 3 конвейера: обучение модели
    │       │   ├── model_trainer.py - шаг 4 конвейера: оценка модели
    │       │   └── results_keeper.py - шаг 5 конвейера: сохранение результатов
    │       ├── logs/ - директория для хранения логов
    │       ├── README.md - описание проекта medic_pipeline (текущий файл)
    │       ├── requirements.txt - перечень установленных пакетов виртуальной среды Python
    │       └── results/ - директория для персистентного хранения данных
    │           ├── logistic_regression_model.pkl - обученная модель логистической регрессии
    │           └── metrics.json - метрики в формате JSON
    └── venv/ - директория для хранения виртуальных сред Python
        └── medic/ - виртуальная среда Python для проекта medic_pipeline
            └── bin/ - бинарные файлы виртуальной среды
                ├── activate - активатор виртуальной среды
                ├── pip - установщик пакетов
                └── python - интерпретатор Python

### Используемые версии библиотек

Виртуальная среда Python: 3.10.18
Apache Airflow: 3.0.2
Также см. в файле requirements.txt

### Шаги конвейера

    Загрузка данных --> Предобработка данных --> Обучение модели --> Оценка модели --> Сохранение результатов

1. Загрузка данных
2. Предобработка данных
3. Обучение модели
4. Оценка модели
5. Сохранение результатов

   В Яндекс.Cloud было создано приложение **medic_pipeline**. По ClientID приложения запрашивается OAuth-токен. Данный токен прописывается в файле config.ini и используется в скрипте при сохранении файлов на удаленном диске.

   Результаты работы конвейера сохраняются на Яндекс.Диск по URL: https://disk.yandex.ru/d/ofnaZFAvsAnuIQ (общий доступ). По пути находятся:

   - Обученная модель логистической регрессии в файле .pkl;
   - Метрики в формате JSON.

### Обработка ошибок

### Логирование

## Запуск проекта из Docker-контейнера
1. Скачиваем из репозитория DockerHub образ Docker

        docker login
        docker pull smozhogin/engineering_image:1.0.X
3. Создаем Docker-контейнер и запускаем в интерактивном режиме

        docker run -it -e LC_ALL=ru_RU.UTF-8 --gpus all --name engineering --hostname engineering -p 8080:8080 engineering_image:1.0.X
4. Активируем виртуальную среду

        source /app/venv/medic/bin/activate
5. Запускаем Apache Airflow

        airflow standalone
6. В браузере на хостовой машине переходим по URL  
   **Логин: admin, пароль: 123**

        http://localhost:8080/dags/Medic_Pipeline
8. Запускаем DAG на исполнение
